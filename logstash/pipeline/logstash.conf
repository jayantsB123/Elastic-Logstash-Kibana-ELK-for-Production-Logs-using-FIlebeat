#
# Production-Grade Logstash Configuration
#
# This configuration is designed to handle multiple log formats
# based on the `realm` field extracted from the top-level XML.
# It uses an efficient, conditional parsing approach.
#

# ==============================================================================
# Input Stage
# ==============================================================================
# Filebeat handles multiline logs. We simply receive the completed events here.
input {
  beats {
    port => 5044
  }
}

# ==============================================================================
# Filter Stage
# ==============================================================================
filter {
  # Drop events that don't contain any XML or are empty.
  if ![message] or [message] !~ "<log" {
    drop {}
  }

  # Use the XML filter to extract top-level attributes like `realm` and `at`.
  # We'll use `store_xml => false` to avoid storing the full XML in memory.
  xml {
    source => "message"
    target => "parsed_xml"
    force_array => false
    remove_namespaces => true
    store_xml => false
    suppress_empty => true
    xpath => [
      "/log/@realm", "realm",
      "/log/@at", "event_timestamp",
      "/log/@lifespan", "lifespan_ms"
    ]
  }

  # Add a tag to easily identify events that have passed the initial parsing.
  mutate {
    add_tag => ["xml_parsed"]
  }

  # --- Conditional Parsing based on 'realm' ---
  # This section applies specific filters to events based on their `realm`.

  if [realm] == "API" {
    # ------------------------------------
    # API Realm Logs
    # ------------------------------------
    # Parse the nested XML content within the `<info>` tag.
    # The `xpath` expressions directly extract the fields we need.
    xml {
      source => "message"
      target => "api_info"
      force_array => false
      remove_namespaces => true
      store_xml => false
      suppress_empty => true
      xpath => [
        # NOTICE THE /text() at the end of each line!
        "/log/info/req/text()", "request_type",        # Gets text inside <req>
        "/log/info/inst/text()", "instance_id",        # Gets text inside <inst>
        "/log/info/api.source/text()", "api_source",   # Gets text inside <api.source>
        "/log/info/source/text()", "client_source_raw", # Gets text inside <source>
        "/log/info/clazz/text()", "dto_class"          # Gets text inside <clazz>
      ]
    }

    # The `<source>` tag contains a JSON string. We need a second filter to
    # parse this nested JSON content into its own structured field.
    if [client_source_raw] {
    json {
      source => "client_source_raw"
      target => "client_source"
      skip_on_invalid_json => true
    }
  }

  # Mutate to clean up fields and add a clear event type.
  mutate {
    add_field => {
      "event_type" => "api_request"
      "log_level" => "INFO"
    }
    remove_field => ["client_source_raw"]
  }

} else if [realm] == "cdci-channel-1" {
  # ------------------------------------
  # Connection Realm Logs - CORRECTED
  # ------------------------------------
  # First, extract the entire text content of the <connect> element
  xml {
    source => "message"
    target => "connect_data"
    force_array => false
    remove_namespaces => true
    store_xml => false
    suppress_empty => true
    xpath => [
      "/log/connect/text()", "connection_text" # Get the text node, not child tags
    ]
  }

  # Now, use a grok filter on the extracted text to get all the details
  grok {
    match => {
      "connection_text" => [
        # This pattern matches the "Try X host:port" line and the "Unable to connect" line.
        "Try\s*%{INT:connection_attempt_number}\s*%{IPORHOST:destination_host}:%{INT:destination_port}%{SPACE}%{IPORHOST:dns_lookup_result}%{SPACE}Unable to connect",
        # Try a simpler pattern if the above fails due to newlines/spacing
        "Try\s*%{INT:connection_attempt_number}\s*%{IPORHOST:destination_host}:%{INT:destination_port}.*Unable to connect"
      ]
    }
    # If grok fails, we can still see the raw text. Remove tag_on_failure for now for debugging.
    # tag_on_failure => ["_grok_parse_failure"]
  }

  mutate {
    add_field => { "event_type" => "connection" }
    add_field => { "log_level" => "WARN" }
    # Clean up the temporary field we created
    remove_field => ["connect_data", "connection_text"]
  }
} else if [realm] == "org.jpos.transaction.TransactionManager" {
  # ------------------------------------
  # Transaction Realm Logs
  # ------------------------------------
  # Add parsing for transaction logs here.
  # For example, you would use an `xml` filter with `xpath` to
  # extract fields from `<commit>` or `<info>` tags.
  xml {
    source => "message"
    target => "transaction"
    force_array => false
    remove_namespaces => true
    store_xml => false
    suppress_empty => true
    xpath => [
      "/log/commit/context/NODE_NAME/text()", "node_name",
      "/log/commit/context/TXN_NAME/text()", "txn_name",
      "/log/commit/context/REQUEST_API/text()", "api_request",
      "/log/commit/context/HTTP_METHOD/text()", "http_method",
      "/log/commit/context/RC/text()", "response_code",
      "/log/commit/context/RAW_REQUEST/text()", "raw_request_json",
      "/log/commit/context/TXN_RESPONSE/text()", "txn_response_json"
    ]
  }
  # Parse the nested JSON for request and response data
  if [raw_request_json] {
  json {
    source => "raw_request_json"
    target => "transaction_data"
    skip_on_invalid_json => true
  }
}
if [txn_response_json] {
json {
  source => "txn_response_json"
  target => "transaction_data.response"
  skip_on_invalid_json => true
}
}
mutate {
  add_field => { "event_type" => "transaction" }
  add_field => { "log_level" => "INFO" }
  remove_field => ["raw_request_json", "txn_response_json"]
}
}

# --- Final Cleanup and Normalization ---

# Normalize the timestamp field
date {
match => [ "event_timestamp", "ISO8601" ]
target => "@timestamp"
timezone => "Asia/Kolkata"
}

# Convert the lifespan to a number and remove temporary fields.
mutate {
convert => { "lifespan_ms" => "integer" }
remove_field => ["message", "parsed_xml", "event_timestamp"]
remove_tag => ["_xml_parse_failure"]
}

# Add a tag for any logs that were not parsed by a specific realm.
if ! [event_type] {
mutate {
  add_field => { "event_type" => "unparsed" }
  add_field => { "log_level" => "UNKNOWN" }
  add_tag => ["_unparsed_log"]
}
}
}

# ==============================================================================
# Output Stage
# ==============================================================================
output {
elasticsearch {
hosts => ["http://elasticsearch:9200"]
index => "jpos1-logs-%{+YYYY.MM.dd}"
user => "elastic"
password => "changeme"
retry_on_conflict => 3
timeout => 60
}

stdout { codec => rubydebug }
}
